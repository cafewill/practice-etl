#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dc-ì§€ì—­ì¹´í…Œê³ ë¦¬.py â€” DiningCode ë¦¬ìŠ¤íŠ¸ JSONì—ì„œ
- addr / road_addr ë¥¼ 3ëìŠ¤(l1,l2,l3)ë¡œ ì ˆë‹¨(ì œì£¼/ì œì£¼íŠ¹ë³„ìì¹˜ë„ â†’ ì œì£¼ë„ í‘œì¤€í™”)
- category ë¥¼ 1ëìŠ¤(ì½¤ë§ˆ ë¶„ë¦¬) ìœ ë‹ˆí¬ ì§‘ê³„
- ê²°ê³¼ ì €ì¥ (ì§€ë²ˆ/ë„ë¡œëª… ë¶„ë¦¬):
    * data/region/regions-addr-triplets.json
    * data/region/regions-road-addr-triplets.json
    * data/region/regions-addr-unique.json
    * data/region/regions-road-addr-unique.json
    * data/category/categories.json

í™”ë©´ ì¶œë ¥:
- ì²˜ë¦¬ ì§„í–‰ ìƒí™©: --log-every N (ê¸°ë³¸ 100ê±´ë§ˆë‹¤)
- name / name_json(ko,en,cn) í”„ë¦¬ë·°: --show-name-json true
- ì§€ì—­ ë¦¬í¬íŠ¸: --show-region-report true

ë²ˆì—­ ì˜µì…˜(ì§€ì—­ì ì¬ì™€ ë™ì¼):
  --translate {auto|romanize|off} (default: romanize)
  --translate-timeout 2.0
  --translate-max 200
  --translate-provider {auto_chain|googletrans|deep} (default: auto_chain)
  --zh-variant {cn|tw}
  --cache-file <path>

ì¹´í…Œê³ ë¦¬ ë²ˆì—­ í’ˆì§ˆ í–¥ìƒ:
- CATEGORY_GLOSSARY_EN/CN ì‚¬ì „ ìš°ì„  ì ìš©
- ì˜ì–´ëŠ” í•­ìƒ Title Caseë¡œ ì •ê·œí™” (BBQ ë“± ì „ë¶€ ëŒ€ë¬¸ì í† í° ë³´ì¡´)

ë¶€íŠ¸ìŠ¤íŠ¸ë© ì¹œì ˆ ë¡œê·¸:
- ì‹œë“œê°€ ì—†ìœ¼ë©´ "skip augment" ì•ˆë‚´ë¥¼ ì¶œë ¥ (ì²« ì‹¤í–‰ì— ìœ ìš©)
- ì‹œë“œê°€ ìˆìœ¼ë©´ top-k / min-count ê¸°ì¤€ìœ¼ë¡œ ì‚¬ì „ ë³´ê°• í›„ ì¶”ê°€/ìŠ¤í‚µ ê±´ìˆ˜ ì¶œë ¥
"""

import argparse
import glob
import json
import os
import re
import sys
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple
from collections import Counter, defaultdict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout

# ---------------- ê³µí†µ ìœ í‹¸ ----------------

def now_local_str() -> str:
    return datetime.now().astimezone().strftime("%Y-%m-%d %H:%M:%S %Z%z")

def _clean_ws(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").replace("\xa0"," ").strip())

def normalize_jeju(text: Optional[str]) -> str:
    if not text:
        return ""
    s = _clean_ws(text)
    s = re.sub(r"(?<![ê°€-í£])ì œì£¼íŠ¹ë³„ìì¹˜ë„(?![ê°€-í£])", "ì œì£¼ë„", s)
    s = re.sub(r"(?<![ê°€-í£])ì œì£¼(?![ê°€-í£])", "ì œì£¼ë„", s)
    s = re.sub(r"(ì œì£¼ë„)(\s+\1)+", r"\1", s)
    return _clean_ws(s)

def gather_files(patterns: List[str]) -> List[str]:
    paths: List[str] = []
    for pat in patterns or []:
        paths.extend(sorted(glob.glob(pat)))
    seen = set(); out=[]
    for p in paths:
        if p not in seen:
            seen.add(p); out.append(p)
    return out

def read_json(path: str) -> Optional[Any]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] JSON ë¡œë“œ ì‹¤íŒ¨: {path} ({e})", file=sys.stderr)
        return None

# ------------- ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ ìë™ íƒìƒ‰ --------------

def get_by_path(doc: Any, path: str) -> Optional[Any]:
    cur = doc
    for key in path.split("."):
        if isinstance(cur, dict) and key in cur:
            cur = cur[key]
        else:
            return None
    return cur

def _looks_like_item_list(v: Any) -> bool:
    return isinstance(v, list) and v and isinstance(v[0], dict) and (
        "nm" in v[0] or "v_rid" in v[0] or "addr" in v[0]
    )

def find_items_auto(doc: Any) -> List[Dict[str, Any]]:
    for path in ("result_data.poi_section.list", "list", "result.list", "data.list"):
        v = get_by_path(doc, path)
        if _looks_like_item_list(v):
            return v  # type: ignore
    found: List[Dict[str, Any]] = []
    def dfs(x: Any):
        nonlocal found
        if found: return
        if isinstance(x, dict):
            for k, vv in x.items():
                if k in ("list","items","pois") and _looks_like_item_list(vv):
                    found = vv  # type: ignore
                    return
                dfs(vv)
        elif isinstance(x, list):
            for it in x:
                dfs(it)
    dfs(doc)
    return found

# ---------------- ì£¼ì†Œ 3ëìŠ¤ íŒŒì„œ ----------------

RE_L2 = re.compile(r"(ì œì£¼ì‹œ|ì„œê·€í¬ì‹œ)$")
RE_L3 = re.compile(r".+(ì|ë©´|ë™)$")
RE_L3_ALT = re.compile(r".+ë¦¬$")

def standardize_l1(l1: str) -> str:
    t = _clean_ws(l1)
    if not t:
        return ""
    if t in ("ì œì£¼", "ì œì£¼íŠ¹ë³„ìì¹˜ë„", "ì œì£¼ë„"):
        return "ì œì£¼ë„"
    if "ì œì£¼" in t and t.endswith("ë„"):
        return "ì œì£¼ë„"
    return t

def split_addr_3(addr: str) -> Tuple[str,str,str]:
    s = normalize_jeju(addr)
    if not s: return ("","","")
    toks = s.split()

    l1 = ""
    for t in toks[:3]:
        if t == "ì œì£¼ë„" or ("ì œì£¼" in t and t.endswith("ë„")):
            l1 = "ì œì£¼ë„"; break

    l2 = ""
    for t in toks:
        if RE_L2.search(t):
            l2 = t; break

    l3 = ""
    start = toks.index(l2)+1 if l2 in toks else 1
    for t in toks[start: start+6]:
        if RE_L3.search(t):
            l3 = t; break
    if not l3:
        for t in toks[start: start+6]:
            if RE_L3_ALT.search(t):
                l3 = t; break

    if not l1 and len(toks) >= 1: l1 = toks[0]
    if not l2 and len(toks) >= 2: l2 = toks[1]
    if not l3 and len(toks) >= 3: l3 = toks[2]

    l1 = standardize_l1(l1)
    l2 = _clean_ws(l2)
    l3 = _clean_ws(l3)
    return (l1, l2, l3)

# ---------------- ì¹´í…Œê³ ë¦¬ ì§‘ê³„ ----------------

def split_categories(cat: Optional[str]) -> List[str]:
    if not cat: return []
    toks = [t.strip() for t in str(cat).split(",")]
    toks = [t for t in toks if t]
    seen=set(); out=[]
    for t in toks:
        if t not in seen:
            seen.add(t); out.append(t)
    return out

# ====== ë²ˆì—­/ë¡œë§ˆì â€” ì§€ì—­ì ì¬ì™€ ë™ì¼ + ì œì£¼ ì§€ëª…/ì¹´í…Œê³ ë¦¬ ë³´ê°• ===================

_RE_HANGUL = re.compile(r"[ê°€-í£]")
_L = ["g","kk","n","d","tt","r","m","b","pp","s","ss","","j","jj","ch","k","t","p","h"]
_V = ["a","ae","ya","yae","eo","e","yeo","ye","o","wa","wae","oe","yo","u","wo","we","wi","yu","eu","ui","i"]
_T = ["","k","k","ks","n","nj","nh","t","l","lk","lm","lb","ls","lt","lp","lh","m","p","ps","t","t","ng","t","t","k","t","p","t"]

_roman_cache: Dict[str, str] = {}
_trans_cache: Dict[Tuple[str, str], str] = {}   # (dest, norm_text) -> translated

def _norm_key(s: str) -> str:
    return re.sub(r"\s+"," ", (s or "").strip()).lower()

def has_hangul(s: str) -> bool:
    return bool(_RE_HANGUL.search(s or ""))

def romanize_korean(text: str) -> str:
    key = _norm_key(text)
    if key in _roman_cache:
        return _roman_cache[key]
    out = []
    for ch in text:
        code = ord(ch)
        if 0xAC00 <= code <= 0xD7A3:
            s_index = code - 0xAC00
            l = s_index // 588
            v = (s_index % 588) // 28
            t = s_index % 28
            out.append(_L[l] + _V[v] + _T[t])
        else:
            out.append(ch)
    s = "".join(out)
    res = " ".join(w.capitalize() for w in re.split(r"\s+", s) if w)
    _roman_cache[key] = res
    return res

def _try_googletrans(text: str, dest: str, src: str = "ko") -> Optional[str]:
    try:
        from googletrans import Translator  # type: ignore
        tr = Translator()
        res = tr.translate(text, src=src, dest=dest)
        return res.text if getattr(res, "text", None) else None
    except Exception:
        return None

def _try_deeptranslator(text: str, dest: str, src: str = "ko") -> Optional[str]:
    try:
        from deep_translator import GoogleTranslator  # type: ignore
        tr = GoogleTranslator(source=src, target=dest)
        return tr.translate(text)
    except Exception:
        return None

def _run_with_timeout(fn, timeout_sec: float, *args, **kwargs) -> Optional[str]:
    with ThreadPoolExecutor(max_workers=1) as ex:
        fut = ex.submit(fn, *args, **kwargs)
        try:
            return fut.result(timeout=timeout_sec)
        except FuturesTimeout:
            return None
        except Exception:
            return None

class TransCtl:
    def __init__(self, provider: str, timeout: float, max_calls: int, zh_variant: str,
                 trace: bool = False):
        self.provider = provider  # auto_chain | googletrans | deep
        self.timeout = timeout
        self.max_calls = max_calls
        self.calls_used = 0
        self.zh_variant = "zh-TW" if zh_variant == "tw" else "zh-CN"
        self.trace = trace

    def _can_call(self) -> bool:
        return self.max_calls is None or self.calls_used < self.max_calls

    def translate(self, text: str, dest: str) -> Optional[str]:
        key = (dest, _norm_key(text))
        if key in _trans_cache:
            return _trans_cache[key]
        if not self._can_call():
            return None
        out: Optional[str] = None
        if self.provider in ("googletrans", "auto_chain"):
            self.calls_used += 1
            if self.trace: print(f"[TX] googletrans â†’ {dest}: '{text}'")
            out = _run_with_timeout(_try_googletrans, self.timeout, text, dest, "ko")
        if not out and self.provider in ("deep", "auto_chain"):
            if not self._can_call():
                return None
            self.calls_used += 1
            if self.trace: print(f"[TX] deep_translator â†’ {dest}: '{text}'")
            out = _run_with_timeout(_try_deeptranslator, self.timeout, text, dest, "ko")
        if out:
            _trans_cache[key] = out
        return out

def load_trans_cache(path: Optional[str]):
    if not path: return
    try:
        with open(path, "r", encoding="utf-8") as f:
            d = json.load(f)
        cnt = 0
        if isinstance(d, dict):
            for dest, m in d.items():
                if not isinstance(m, dict): continue
                for norm_text, val in m.items():
                    _trans_cache[(dest, norm_text)] = val
                    cnt += 1
        print(f"[CACHE] ë²ˆì—­ ìºì‹œ ë¡œë“œ: {cnt}ê±´ from {path}")
    except FileNotFoundError:
        print(f"[CACHE] íŒŒì¼ ì—†ìŒ(ìƒˆë¡œ ìƒì„± ì˜ˆì •): {path}")
    except Exception as e:
        print(f"[CACHE] ë¡œë“œ ì‹¤íŒ¨({e}) â€” ë¬´ì‹œí•˜ê³  ì§„í–‰")

def save_trans_cache(path: Optional[str]):
    if not path: return
    out: Dict[str, Dict[str, str]] = {}
    for (dest, norm_text), val in _trans_cache.items():
        out.setdefault(dest, {})[norm_text] = val
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
    except Exception:
        pass
    with open(path, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print(f"[CACHE] ë²ˆì—­ ìºì‹œ ì €ì¥: {len(_trans_cache)}ê±´ -> {path}")

# ---- ê°„â†”ë²ˆ ë³€í™˜(ì¹´í…Œê³ ë¦¬ì—ë„ ì‚¬ìš©) ---------------------------------------------

CN_TO_TW_TABLE = str.maketrans({
    "æµ": "æ¿Ÿ", "å²›": "å³¶", "å½’": "æ­¸", "æ—§": "èˆŠ", "é™": "éœ",
    "å¸‚": "å¸‚", "é“": "é“", "éƒ¡": "éƒ¡", "é‚‘": "é‚‘", "æ´": "æ´", "é‡Œ": "é‡Œ",
    "é¢": "éºµ",
})

def to_tw_if_needed(text_cn: str, zh_variant: str) -> str:
    return text_cn.translate(CN_TO_TW_TABLE) if zh_variant == "tw" else text_cn

# ---- ì˜ì–´ Title Case ì •ê·œí™” ----------------------------------------------------

def titlecase_en(s: str) -> str:
    if not s: return s
    parts = re.split(r"([/\-\s])", s)  # êµ¬ë¶„ì ë³´ì¡´
    def cap(tok: str) -> str:
        if not tok or tok in "/- ":
            return tok
        if tok.isupper() and len(tok) <= 4:
            return tok
        return tok[:1].upper() + tok[1:]
    return "".join(cap(p) for p in parts)

# ---- ì œì£¼ ì§€ëª… ì „ìš© ì¤‘êµ­ì–´ ìš©ì–´ì§‘ & ê·œì¹™ ---------------------------------------

JEJU_ZH_GLOSSARY_FULL_CN: Dict[str, str] = {
    "ì œì£¼ë„": "æµå·å²›",
    "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "æµå·ç‰¹åˆ«è‡ªæ²»é“",
    "ì œì£¼ì‹œ": "æµå·å¸‚",
    "ì„œê·€í¬ì‹œ": "è¥¿å½’æµ¦å¸‚",
    "ì¤‘ë¬¸ë™": "ä¸­æ–‡æ´",
    "ì„±ì‚°ì": "åŸå±±é‚‘",
    "ì• ì›”ì": "æ¶¯æœˆé‚‘",
    "êµ¬ì¢Œì": "æ—§å·¦é‚‘",
    "ì¡°ì²œì": "æœå¤©é‚‘",
    "í•œë¦¼ì": "ç¿°æ—é‚‘",
    "ë‚¨ì›ì": "å—å…ƒé‚‘",
    "ëŒ€ì •ì": "å¤§é™é‚‘",
    "í‘œì„ ë©´": "è¡¨å–„é¢",
    "ì•ˆë•ë©´": "å®‰å¾·é¢",
    "ìš°ë„": "ç‰›å²›",
    "ìš°ë„ë©´": "ç‰›å²›é¢",
}
JEJU_ZH_GLOSSARY_ROOT_CN: Dict[str, str] = {
    "ì œì£¼": "æµå·", "ì„œê·€í¬": "è¥¿å½’æµ¦", "ì¤‘ë¬¸": "ä¸­æ–‡", "ì„±ì‚°": "åŸå±±",
    "ì• ì›”": "æ¶¯æœˆ", "êµ¬ì¢Œ": "æ—§å·¦", "ì¡°ì²œ": "æœå¤©", "í•œë¦¼": "ç¿°æ—",
    "ë‚¨ì›": "å—å…ƒ", "ëŒ€ì •": "å¤§é™", "í‘œì„ ": "è¡¨å–„", "ì•ˆë•": "å®‰å¾·", "ìš°ë„": "ç‰›å²›",
}
ZH_SUFFIX_MAP = {"ë„":"é“","ì‹œ":"å¸‚","êµ°":"éƒ¡","ì":"é‚‘","ë©´":"é¢","ë™":"æ´","ë¦¬":"é‡Œ"}

def jeju_ko_to_cn(ko: str, zh_variant: str, transctl: Optional[TransCtl]) -> Optional[str]:
    src = (ko or "").strip()
    if not src or not has_hangul(src):
        return None
    hit = JEJU_ZH_GLOSSARY_FULL_CN.get(src)
    if hit:
        return to_tw_if_needed(hit, zh_variant)
    for suf in ("íŠ¹ë³„ìì¹˜ë„","ì œì£¼ë„","ë„","ì‹œ","êµ°","ì","ë©´","ë™","ë¦¬"):
        if src.endswith(suf):
            root = src[:-len(suf)]
            suffix_cn = ZH_SUFFIX_MAP.get(suf, "")
            mapped_root = JEJU_ZH_GLOSSARY_ROOT_CN.get(root)
            if mapped_root:
                return to_tw_if_needed(mapped_root + suffix_cn, zh_variant)
            break
    if transctl:
        dest = "zh-TW" if zh_variant == "tw" else "zh-CN"
        t = transctl.translate(src, dest)
        if t: return t.strip()
    return None

# ---- ì¹´í…Œê³ ë¦¬ ì „ìš© ì‚¬ì „ --------------------------------------------------------

CATEGORY_GLOSSARY_EN: Dict[str, str] = {
    # Cafe & Dessert
    "ë””ì €íŠ¸": "Dessert", "ì»¤í”¼": "Coffee", "ì¹´í˜": "Cafe", "ë¸ŒëŸ°ì¹˜": "Brunch",
    "ë² ì´ì»¤ë¦¬": "Bakery", "ë„ë„›": "Donut", "ì¼€ì´í¬": "Cake", "ì†Œê¸ˆë¹µ": "Salt Bread",
    "í¬ë¡œí”Œ": "Croffle", "ì™€í”Œ": "Waffle", "ë¹™ìˆ˜": "Bingsu", "ì•„ì´ìŠ¤í¬ë¦¼": "Ice Cream",
    "ì ¤ë¼ë˜": "Gelato", "ë¶ì¹´í˜": "Book Cafe", "ë£¨í”„íƒ‘": "Rooftop Cafe", "ì™€ì¸ë°”": "Wine Bar",
    # Korean & Local
    "êµ­ë°¥": "Gukbap", "í•´ì¥êµ­": "Haejang-guk", "ê¹€ë°¥": "Gimbap", "ë–¡ë³¶ì´": "Tteokbokki",
    "ë§Œë‘": "Dumplings", "ë¹„ë¹”ë°¥": "Bibimbap", "ë¶ˆê³ ê¸°": "Bulgogi", "ì •ìœ¡ì‹ë‹¹": "Butcher's BBQ",
    "ë”ë² ê³ ê¸°": "Dombae Pork", "ê·¼ê³ ê¸°": "Thick-cut Pork",
    # Noodles
    "êµ­ìˆ˜": "Noodles", "ì¹¼êµ­ìˆ˜": "Kalguksu", "ë©”ë°€êµ­ìˆ˜": "Buckwheat Noodles",
    "ìš°ë™": "Udon", "ë¼ë©˜": "Ramen", "ë¼ë©´": "Ramen (Korean Style)",
    # Japanese
    "ìŠ¤ì‹œ": "Sushi", "ì´ˆë°¥": "Sushi", "ì˜¤ë§ˆì¹´ì„¸": "Omakase",
    "í…ë™": "Tendon", "ê·œë™": "Gyudon",
    # Western
    "íŒŒìŠ¤íƒ€": "Pasta", "í”¼ì": "Pizza", "ìˆ˜ì œë²„ê±°": "Handmade Burger",
    "í–„ë²„ê±°": "Burger", "ìŠ¤í…Œì´í¬": "Steak", "ë¹„ìŠ¤íŠ¸ë¡œ": "Bistro",
    # Seafood / Jeju
    "ë¬¼íšŒ": "Mulhoe", "í•´ë¬¼íƒ•": "Seafood Hot Pot", "í•´ë¬¼ëšë°°ê¸°": "Seafood Earthen Pot",
    "ì „ë³µì£½": "Abalone Porridge", "ì „ë³µëŒì†¥ë°¥": "Abalone Hot-pot Rice", "ì „ë³µì†¥ë°¥": "Abalone Pot Rice",
    "ì „ë³µêµ¬ì´": "Grilled Abalone", "ê°ˆì¹˜êµ¬ì´": "Grilled Hairtail", "ê³ ë“±ì–´êµ¬ì´": "Grilled Mackerel",
    "ì˜¥ë”êµ¬ì´": "Grilled Tilefish", "ê°ˆì¹˜ì¡°ë¦¼": "Braised Hairtail", "ê°ˆì¹˜êµ­": "Hairtail Soup",
    "ë”±ìƒˆìš°": "Sweet Shrimp", "ë”±ìƒˆìš°íšŒ": "Sweet Shrimp Sashimi", "ì„±ê²Œë¹„ë¹”ë°¥": "Sea Urchin Bibimbap",
    "ì„±ê²Œë¯¸ì—­êµ­": "Sea Urchin Seaweed Soup", "ë³´ë§ì£½": "Top Shell Porridge", "ë³´ë§ì¹¼êµ­ìˆ˜": "Top Shell Kalguksu",
    # Drinks / Bars
    "ë§¥ì£¼": "Beer", "í•˜ì´ë³¼": "Highball", "í¬ì°¨": "Pocha", "í": "Pub", "ìˆ ì§‘": "Bar", "ìš”ë¦¬ì£¼ì ": "Gastro Pub",
    # Chinese
    "ë§ˆë¼íƒ•": "Mala Soup", "ì§¬ë½•": "Jjamppong", "ì§œì¥ë©´": "Jjajangmyeon", "íƒ•ìˆ˜ìœ¡": "Sweet & Sour Pork",
    # Chicken / Meat
    "ì¹˜í‚¨": "Chicken", "ì‚¼ê²¹ì‚´": "Samgyeopsal", "ì˜¤ê²¹ì‚´": "Ogeopsal",
    "ë¼ì§€ê°ˆë¹„": "Pork Ribs", "ì–‘ë…ê°ˆë¹„": "Marinated Ribs", "ìš°ëŒ€ê°ˆë¹„": "Premium Beef Ribs",
    "ì†Œê°ˆë¹„ì‚´": "Beef Rib Finger",
}
CATEGORY_GLOSSARY_CN: Dict[str, str] = {
    "ë””ì €íŠ¸": "ç”œç‚¹", "ì»¤í”¼": "å’–å•¡", "ì¹´í˜": "å’–å•¡åº—", "ë¸ŒëŸ°ì¹˜": "æ—©åˆé¤",
    "ë² ì´ì»¤ë¦¬": "çƒ˜ç„™åº—", "ë„ë„›": "ç”œç”œåœˆ", "ì¼€ì´í¬": "è›‹ç³•", "ì†Œê¸ˆë¹µ": "ç›é¢åŒ…",
    "í¬ë¡œí”Œ": "å¯é¢‚åå¤«", "ì™€í”Œ": "åå¤«é¥¼", "ë¹™ìˆ˜": "åˆ¨å†°", "ì•„ì´ìŠ¤í¬ë¦¼": "å†°æ·‡æ·‹",
    "ì ¤ë¼ë˜": "æ„å¼å†°æ·‡æ·‹", "ë¶ì¹´í˜": "ä¹¦åº—å’–å•¡", "ë£¨í”„íƒ‘": "éœ²å°å’–å•¡", "ì™€ì¸ë°”": "è‘¡è„é…’å§",
    "êµ­ë°¥": "æ±¤é¥­", "í•´ì¥êµ­": "è§£é…’æ±¤", "ê¹€ë°¥": "ç´«èœåŒ…é¥­", "ë–¡ë³¶ì´": "ç‚’å¹´ç³•",
    "ë§Œë‘": "é¥ºå­", "ë¹„ë¹”ë°¥": "æ‹Œé¥­", "ë¶ˆê³ ê¸°": "çƒ¤è‚‰ï¼ˆéŸ©å¼ï¼‰", "ì •ìœ¡ì‹ë‹¹": "è‚‰é“ºç›´çƒ¤",
    "ë”ë² ê³ ê¸°": "åˆ‡ç‰‡çŒªè‚‰", "ê·¼ê³ ê¸°": "åšåˆ‡çŒªè‚‰",
    "êµ­ìˆ˜": "é¢", "ì¹¼êµ­ìˆ˜": "åˆ€å‰Šé¢", "ë©”ë°€êµ­ìˆ˜": "èéº¦é¢",
    "ìš°ë™": "ä¹Œå†¬é¢", "ë¼ë©˜": "æ‹‰é¢", "ë¼ë©´": "æ³¡é¢",
    "ìŠ¤ì‹œ": "å¯¿å¸", "ì´ˆë°¥": "å¯¿å¸", "ì˜¤ë§ˆì¹´ì„¸": "ä¸»å¨ç²¾é€‰",
    "í…ë™": "å¤©ä¸¼", "ê·œë™": "ç‰›è‚‰ç›–é¥­",
    "íŒŒìŠ¤íƒ€": "æ„é¢", "í”¼ì": "æŠ«è¨", "ìˆ˜ì œë²„ê±°": "æ‰‹å·¥æ±‰å ¡",
    "í–„ë²„ê±°": "æ±‰å ¡", "ìŠ¤í…Œì´í¬": "ç‰›æ’", "ë¹„ìŠ¤íŠ¸ë¡œ": "å°é…’é¦†",
    "ë¬¼íšŒ": "å‡‰æ‹Œç”Ÿé±¼æ±¤", "í•´ë¬¼íƒ•": "æµ·é²œæ±¤", "í•´ë¬¼ëšë°°ê¸°": "æµ·é²œç ‚é”…",
    "ì „ë³µì£½": "é²é±¼ç²¥", "ì „ë³µëŒì†¥ë°¥": "é²é±¼çŸ³é”…æ‹Œé¥­", "ì „ë³µì†¥ë°¥": "é²é±¼ç ‚é”…é¥­",
    "ì „ë³µêµ¬ì´": "çƒ¤é²é±¼", "ê°ˆì¹˜êµ¬ì´": "çƒ¤å¸¦é±¼", "ê³ ë“±ì–´êµ¬ì´": "çƒ¤é’èŠ±é±¼",
    "ì˜¥ë”êµ¬ì´": "çƒ¤æ¡çŸ³é²·", "ê°ˆì¹˜ì¡°ë¦¼": "ç‚–å¸¦é±¼", "ê°ˆì¹˜êµ­": "å¸¦é±¼æ±¤",
    "ë”±ìƒˆìš°": "ç”œè™¾", "ë”±ìƒˆìš°íšŒ": "ç”œè™¾ç”Ÿé±¼ç‰‡", "ì„±ê²Œë¹„ë¹”ë°¥": "æµ·èƒ†æ‹Œé¥­",
    "ì„±ê²Œë¯¸ì—­êµ­": "æµ·èƒ†æµ·å¸¦æ±¤", "ë³´ë§ì£½": "æ³•èºç²¥", "ë³´ë§ì¹¼êµ­ìˆ˜": "æ³•èºåˆ€å‰Šé¢",
    "ë§¥ì£¼": "å•¤é…’", "í•˜ì´ë³¼": "é«˜çƒé…’", "í¬ì°¨": "è·¯è¾¹æ‘Šé…’é¦†", "í": "å•¤é…’å±‹", "ìˆ ì§‘": "å°é…’å§", "ìš”ë¦¬ì£¼ì ": "æ–™ç†é…’é¦†",
    "ë§ˆë¼íƒ•": "éº»è¾£çƒ«", "ì§¬ë½•": "ä»€é”¦æµ·é²œé¢", "ì§œì¥ë©´": "ç‚¸é…±é¢", "íƒ•ìˆ˜ìœ¡": "ç³–é†‹è‚‰",
    "ì¹˜í‚¨": "ç‚¸é¸¡", "ì‚¼ê²¹ì‚´": "äº”èŠ±è‚‰", "ì˜¤ê²¹ì‚´": "äº”èŠ±ä¸‰å±‚è‚‰",
    "ë¼ì§€ê°ˆë¹„": "çŒªæ’éª¨", "ì–‘ë…ê°ˆë¹„": "è…Œåˆ¶æ’éª¨", "ìš°ëŒ€ê°ˆë¹„": "åšåˆ‡ç‰›æ’éª¨",
    "ì†Œê°ˆë¹„ì‚´": "ç‰›è‚‹æ¡",
}

def _category_dict_lookup(ko: str, zh_variant: str) -> Tuple[Optional[str], Optional[str]]:
    if not ko: return (None, None)
    en = CATEGORY_GLOSSARY_EN.get(ko)
    cn = CATEGORY_GLOSSARY_CN.get(ko)
    if cn: cn = to_tw_if_needed(cn, zh_variant)
    return (en, cn)

def _category_slashed_lookup(ko: str, zh_variant: str) -> Tuple[Optional[str], Optional[str]]:
    if "/" not in ko:
        return (None, None)
    parts = [p.strip() for p in ko.split("/")]
    en_parts, cn_parts = [], []
    for p in parts:
        en_p, cn_p = _category_dict_lookup(p, zh_variant)
        en_parts.append(en_p or p)
        cn_parts.append(cn_p or p)
    en = " / ".join(titlecase_en(x) for x in en_parts)
    cn = " / ".join(cn_parts)
    return (en, to_tw_if_needed(cn, zh_variant))

# ---------------- ì´ë¦„ JSON ë¹Œë” ----------------

def build_name_obj_region(ko: str, translate_mode: str,
                          transctl: Optional[TransCtl], zh_variant: str) -> Dict[str,str]:
    ko = (ko or "").strip()
    if not ko:
        return {"ko":"", "en":"", "cn":""}

    if translate_mode == "off":
        en = ""
    elif translate_mode == "romanize" or transctl is None:
        en = romanize_korean(ko) if has_hangul(ko) else ko
    else:
        if has_hangul(ko):
            t_en = transctl.translate(ko, "en")
            en = (t_en or "").strip() or romanize_korean(ko)
        else:
            en = ko
    en = titlecase_en(en)

    if translate_mode == "off":
        cn = ""
    else:
        jeju_cn = jeju_ko_to_cn(ko, zh_variant, transctl if translate_mode == "auto" else None)
        if jeju_cn:
            cn = jeju_cn
        else:
            if translate_mode == "romanize" or transctl is None:
                cn = en if en else (romanize_korean(ko) if has_hangul(ko) else ko)
            else:
                dest = "zh-TW" if zh_variant == "tw" else "zh-CN"
                t_zh = transctl.translate(ko, dest) if has_hangul(ko) else None
                cn = (t_zh or (en if en else romanize_korean(ko))).strip()
                cn = to_tw_if_needed(cn, zh_variant)
    return {"ko": ko, "en": en, "cn": cn}

def build_name_obj_category(ko: str, translate_mode: str,
                            transctl: Optional[TransCtl], zh_variant: str) -> Dict[str,str]:
    ko = (ko or "").strip()
    if not ko:
        return {"ko":"", "en":"", "cn":""}

    en = None
    cn = None

    en, cn = _category_dict_lookup(ko, zh_variant)

    if en is None and cn is None and "/" in ko:
        en, cn = _category_slashed_lookup(ko, zh_variant)

    if en is None or cn is None:
        if translate_mode == "off":
            en = en if en is not None else ""
            cn = cn if cn is not None else ""
        elif translate_mode == "romanize" or transctl is None:
            en_fb = romanize_korean(ko) if has_hangul(ko) else ko
            cn_fb = en_fb
            en = en if en is not None else en_fb
            cn = cn if cn is not None else cn_fb
        else:
            if en is None:
                t_en = transctl.translate(ko, "en") if has_hangul(ko) else None
                en = (t_en or romanize_korean(ko) if has_hangul(ko) else ko)
            if cn is None:
                dest = "zh-TW" if zh_variant == "tw" else "zh-CN"
                t_zh = transctl.translate(ko, dest) if has_hangul(ko) else None
                cn = (t_zh or (en or romanize_korean(ko))).strip()
                cn = to_tw_if_needed(cn, zh_variant)

    en = titlecase_en(en or "")
    return {"ko": ko, "en": en, "cn": cn or ""}

# ---------------- ì‹œë“œ ë³´ê°• (ì¹œì ˆ ë¡œê·¸ í¬í•¨) ----------------

def _pretty_seed(path: Optional[str]) -> str:
    return path if path else "(none)"

def augment_glossary_from_category_seed(seed_path: str,
                                        translate_mode: str,
                                        transctl: Optional[TransCtl],
                                        zh_variant: str,
                                        top_k: int,
                                        min_count: int) -> Tuple[int,int]:
    """categories.json ì‹œë“œì—ì„œ ìƒìœ„ í† í°ì„ ì‚¬ì „ì— ì£¼ì…."""
    data = read_json(seed_path)
    if not isinstance(data, list):
        print(f"[SEED] category-seed í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ â†’ skip", file=sys.stderr)
        return (0,0)
    # ì •ë ¬(ë¹ˆë„ ë‚´ë¦¼ì°¨ìˆœ) í›„ top-k + min-count
    rows = [x for x in data if isinstance(x, dict) and x.get("category")]
    rows.sort(key=lambda x: (-int(x.get("count", 0)), str(x.get("category"))))
    rows = [r for r in rows if int(r.get("count", 0)) >= int(min_count)]
    rows = rows[: int(top_k)]
    added = skipped = 0
    for r in rows:
        ko = str(r["category"]).strip()
        if ko in CATEGORY_GLOSSARY_EN and ko in CATEGORY_GLOSSARY_CN:
            skipped += 1
            continue
        nj = build_name_obj_category(ko, translate_mode, transctl, zh_variant)
        if ko not in CATEGORY_GLOSSARY_EN and nj.get("en"):
            CATEGORY_GLOSSARY_EN[ko] = nj["en"]
        if ko not in CATEGORY_GLOSSARY_CN and nj.get("cn"):
            CATEGORY_GLOSSARY_CN[ko] = nj["cn"]
        added += 1
    return (added, skipped)

def augment_jeju_from_region_seed(seed_path: str,
                                  translate_mode: str,
                                  transctl: Optional[TransCtl],
                                  zh_variant: str,
                                  top_k: int,
                                  min_count: int) -> Tuple[int,int]:
    """
    regions-addr-unique.json ì‹œë“œì—ì„œ l1/l2/l3 í…ìŠ¤íŠ¸ë¥¼ ìŠ¤ìº”í•´
    JEJU_ZH_GLOSSARY_FULL_CNì— ì—†ìœ¼ë©´ ëŸ°íƒ€ì„ ë³´ê°•(ì†ë„/ì¼ê´€ì„± í–¥ìƒ).
    """
    data = read_json(seed_path)
    if not isinstance(data, list):
        print(f"[SEED] region-seed í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ â†’ skip", file=sys.stderr)
        return (0,0)
    rows = [x for x in data if isinstance(x, dict)]
    rows.sort(key=lambda x: (-int(x.get("count", 0)), str(x.get("l1","")), str(x.get("l2","")), str(x.get("l3",""))))
    rows = [r for r in rows if int(r.get("count", 0)) >= int(min_count)]
    rows = rows[: int(top_k)]

    uniq_terms: List[str] = []
    seen = set()
    for r in rows:
        for key in ("l1","l2","l3"):
            t = (r.get(key) or "").strip()
            if t and t not in seen:
                seen.add(t); uniq_terms.append(t)

    added = skipped = 0
    for ko in uniq_terms:
        if ko in JEJU_ZH_GLOSSARY_FULL_CN:
            skipped += 1
            continue
        nj = build_name_obj_region(ko, translate_mode, transctl, zh_variant)
        cn = nj.get("cn","")
        if cn:
            JEJU_ZH_GLOSSARY_FULL_CN[ko] = cn
            added += 1
        else:
            skipped += 1
    return (added, skipped)

# ---------------- ì§‘ê³„ ì²˜ë¦¬ (+ ì§„í–‰ ë¡œê·¸) ----------------

def process_files(paths: List[str], dot_path: Optional[str], log_every: int,
                  transctl: Optional[TransCtl]):
    regions_addr_per_shop: List[Dict[str, Any]] = []
    regions_road_per_shop: List[Dict[str, Any]] = []
    addr_triplet_counter = defaultdict(int)
    road_triplet_counter = defaultdict(int)
    category_counter: Counter = Counter()

    total_items = 0
    for p in paths:
        doc = read_json(p)
        if doc is None:
            continue
        items = get_by_path(doc, dot_path) if dot_path else find_items_auto(doc)
        if not (isinstance(items, list) and items and isinstance(items[0], dict)):
            items = find_items_auto(doc)
        total_items += len(items or [])

    done = 0
    t0 = time.perf_counter()
    le = max(1, int(log_every or 100))

    for p in paths:
        doc = read_json(p)
        if doc is None:
            continue
        items = get_by_path(doc, dot_path) if dot_path else find_items_auto(doc)
        if not (isinstance(items, list) and items and isinstance(items[0], dict)):
            items = find_items_auto(doc)

        for it in (items or []):
            if not isinstance(it, dict):
                continue
            rid = (it.get("v_rid") or "").strip()

            addr = normalize_jeju(it.get("addr"))
            l1a,l2a,l3a = split_addr_3(addr) if addr else ("","","")
            regions_addr_per_shop.append({
                "v_rid": rid,
                "addr3": {"l1": l1a, "l2": l2a, "l3": l3a},
            })
            if l1a or l2a or l3a:
                addr_triplet_counter[(l1a,l2a,l3a)] += 1

            raddr = normalize_jeju(it.get("road_addr") or it.get("roadAddress") or it.get("road_addr_full"))
            l1r,l2r,l3r = split_addr_3(raddr) if raddr else ("","","")
            regions_road_per_shop.append({
                "v_rid": rid,
                "road_addr3": {"l1": l1r, "l2": l2r, "l3": l3r},
            })
            if l1r or l2r or l3r:
                road_triplet_counter[(l1r,l2r,l3r)] += 1

            for tok in split_categories(it.get("category")):
                category_counter[tok] += 1

            done += 1
            if (done % le == 0) or (done == total_items):
                elapsed = time.perf_counter() - t0
                pct = (done / total_items * 100.0) if total_items else 100.0
                calls = getattr(transctl, "calls_used", None) if transctl else None
                extra = f", tx_calls={calls}" if calls is not None else ""
                print(f"[PROG] scan {done}/{total_items} ({pct:.1f}%) elapsed={elapsed:.1f}s{extra}")

    return (regions_addr_per_shop, regions_road_per_shop,
            addr_triplet_counter, road_triplet_counter, category_counter)

# ---------------- í”„ë¦¬ë·° ì¶œë ¥ ----------------

def _fmt_name_line(tag: str, obj: Dict[str,str], extra: str = "") -> str:
    return f"{tag:8s} | ko: {obj.get('ko','')} | en: {obj.get('en','')} | cn: {obj.get('cn','')}{extra}"

def print_regions_preview(addr_rows: List[Dict[str, Any]],
                          road_rows: List[Dict[str, Any]],
                          limit: int = 10):
    print("\n[PREVIEW] Regions - addr (v_rid | l1/l2/l3)")
    print("v_rid           | addr(l1/l2/l3)")
    print("-"*56)
    for r in addr_rows[:limit]:
        a = r["addr3"]
        addr_s = f"{a['l1']} {a['l2']} {a['l3']}".strip()
        print(f"{(r['v_rid'] or ''):15s} | {addr_s:30s}")
    if len(addr_rows) > limit:
        print(f"... ({len(addr_rows)-limit} more)")

    print("\n[PREVIEW] Regions - road_addr (v_rid | l1/l2/l3)")
    print("v_rid           | road_addr(l1/l2/l3)")
    print("-"*60)
    for r in road_rows[:limit]:
        a = r["road_addr3"]
        addr_s = f"{a['l1']} {a['l2']} {a['l3']}".strip()
        print(f"{(r['v_rid'] or ''):15s} | {addr_s:30s}")
    if len(road_rows) > limit:
        print(f"... ({len(road_rows)-limit} more)")

def print_regions_namejson_preview(addr_triplet_counter: Dict[Tuple[str,str,str], int],
                                   translate_mode: str,
                                   transctl: Optional[TransCtl],
                                   zh_variant: str,
                                   limit_per_level: int = 10):
    l1_cnt = defaultdict(int)
    l2_cnt = defaultdict(int)
    l3_cnt = dict(addr_triplet_counter)

    for (l1,l2,l3), c in addr_triplet_counter.items():
        if l1: l1_cnt[l1] += c
        if l1 and l2: l2_cnt[(l1,l2)] += c

    def top(d, n):
        return list(sorted(d.items(), key=lambda x: (-x[1], x[0])))[:n]

    print("\n[NAMEJSON] Regions L1 (top)")
    for l1, c in top(l1_cnt, limit_per_level):
        nj = build_name_obj_region(l1, translate_mode, transctl, zh_variant)
        print(_fmt_name_line("L1", nj, extra=f"  (count={c})"))

    print("\n[NAMEJSON] Regions L2 (top)")
    for (l1,l2), c in top(l2_cnt, limit_per_level):
        nj1 = build_name_obj_region(l1, translate_mode, transctl, zh_variant)
        nj2 = build_name_obj_region(l2, translate_mode, transctl, zh_variant)
        print(_fmt_name_line("L1", nj1, extra=f"  >"))
        print(_fmt_name_line("L2", nj2, extra=f"  (count={c})"))

    print("\n[NAMEJSON] Regions L3 (top)")
    l3_pairs = [((l1,l2,l3), c) for (l1,l2,l3), c in l3_cnt.items() if l3]
    l3_pairs.sort(key=lambda x: (-x[1], x[0][0], x[0][1], x[0][2]))
    for (l1,l2,l3), c in l3_pairs[:limit_per_level]:
        nj1 = build_name_obj_region(l1, translate_mode, transctl, zh_variant)
        nj2 = build_name_obj_region(l2, translate_mode, transctl, zh_variant)
        nj3 = build_name_obj_region(l3, translate_mode, transctl, zh_variant)
        print(_fmt_name_line("L1", nj1, extra="  >"))
        print(_fmt_name_line("L2", nj2, extra="  >"))
        print(_fmt_name_line("L3", nj3, extra=f"  (count={c})"))
        print("-")

def print_categories_preview(counter: Counter, limit: int = 30):
    print("\n[PREVIEW] Categories (token : count)")
    for tok, cnt in counter.most_common(limit):
        print(f"- {tok} : {cnt}")
    rest = len(counter) - min(limit, len(counter))
    if rest > 0:
        print(f"... (+{rest} more unique)")

def print_categories_namejson_preview(counter: Counter,
                                      translate_mode: str,
                                      transctl: Optional[TransCtl],
                                      zh_variant: str,
                                      limit: int = 20):
    print("\n[NAMEJSON] Categories (top)")
    for tok, cnt in counter.most_common(limit):
        nj = build_name_obj_category(tok, translate_mode, transctl, zh_variant)
        print(_fmt_name_line("CAT", nj, extra=f"  (count={cnt})"))

def print_region_report(addr_triplet_counter: Dict[Tuple[str,str,str], int],
                        limit_l1: int, limit_l2: int, limit_l3: int):
    print("\n[REPORT] Region counts")
    l1_cnt = defaultdict(int)
    l2_cnt = defaultdict(int)
    l3_cnt = defaultdict(int)
    for (l1,l2,l3), c in addr_triplet_counter.items():
        if l1: l1_cnt[l1] += c
        if l1 and l2: l2_cnt[(l1,l2)] += c
        if l1 and l2 and l3: l3_cnt[(l1,l2,l3)] += c

    def top(d, n):
        return list(sorted(d.items(), key=lambda x: (-x[1], x[0])))[:n]

    print("  - L1 top:")
    for l1, c in top(l1_cnt, limit_l1):
        print(f"    * {l1} : {c}")

    print("  - L2 top:")
    for (l1,l2), c in top(l2_cnt, limit_l2):
        print(f"    * {l1} > {l2} : {c}")

    print("  - L3 top:")
    for (l1,l2,l3), c in top(l3_cnt, limit_l3):
        print(f"    * {l1} > {l2} > {l3} : {c}")

def save_json(obj: Any, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    n = len(obj) if isinstance(obj, (list, dict)) else 1
    print(f"[OK] ì €ì¥: {path} (items={n})")

# ---------------- CLI ----------------

def str2bool(v) -> bool:
    if isinstance(v, bool):
        return v
    return str(v).strip().lower() in ("1","true","t","yes","y","on")

def main():
    start = time.perf_counter()
    print(f"[TIME] ì‹œì‘: {now_local_str()}")

    ap = argparse.ArgumentParser(
        description="addr/road_addr 3ëìŠ¤ ë¶„í•´(ì œì£¼ í‘œì¤€í™”) + category ìœ ë‹ˆí¬ ì§‘ê³„ (basic.json ìƒì„± ì—†ìŒ)"
    )
    ap.add_argument("--files", nargs="+", required=True, help="ì…ë ¥ JSON íŒŒì¼/íŒ¨í„´(ê³µë°± êµ¬ë¶„)")
    ap.add_argument("--path", default=None, help="ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ dot-path (ì˜ˆ: result_data.poi_section.list)")

    # í™”ë©´ ì¶œë ¥
    ap.add_argument("--show", default="true", choices=["true","false"], help="ì½˜ì†” ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸ true)")
    ap.add_argument("--show-name-json", default="true", choices=["true","false"], help="name_json(ko/en/cn) ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥")
    ap.add_argument("--show-region-report", default="false", choices=["true","false"], help="L1/L2/L3 ìƒìœ„ ë¦¬í¬íŠ¸ ì¶œë ¥")
    ap.add_argument("--name-limit-reg", type=int, default=10, help="ì§€ì—­ L1/L2/L3 í”„ë¦¬ë·° ê°œìˆ˜")
    ap.add_argument("--name-limit-cat", type=int, default=20, help="ì¹´í…Œê³ ë¦¬ í”„ë¦¬ë·° ê°œìˆ˜")
    ap.add_argument("--report-limit-l1", type=int, default=10)
    ap.add_argument("--report-limit-l2", type=int, default=10)
    ap.add_argument("--report-limit-l3", type=int, default=10)

    # ì§„í–‰ ë¡œê·¸
    ap.add_argument("--log-every", type=int, default=100, help="Nê±´ë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê·¸ ì¶œë ¥ (ê¸°ë³¸ 100)")

    # ì €ì¥ ì˜µì…˜
    ap.add_argument("--save-region", default="true", choices=["true","false"], help="ì§€ì—­ ê²°ê³¼ ì €ì¥ (ê¸°ë³¸ true)")
    ap.add_argument("--save-category", default="true", choices=["true","false"], help="ì¹´í…Œê³ ë¦¬ ê²°ê³¼ ì €ì¥ (ê¸°ë³¸ true)")
    ap.add_argument("--out-addr-triplets", default="data/region/regions-addr-triplets.json")
    ap.add_argument("--out-road-triplets", default="data/region/regions-road-addr-triplets.json")
    ap.add_argument("--out-addr-unique", default="data/region/regions-addr-unique.json")
    ap.add_argument("--out-road-unique", default="data/region/regions-road-addr-unique.json")
    ap.add_argument("--out-category", default="data/category/categories.json")

    # ë²ˆì—­ ì˜µì…˜(ì§€ì—­ì ì¬ì™€ ë™ì¼)
    ap.add_argument("--translate", choices=["auto","romanize","off"], default="romanize")
    ap.add_argument("--translate-timeout", type=float, default=2.0)
    ap.add_argument("--translate-max", type=int, default=200)
    ap.add_argument("--translate-provider", choices=["auto_chain","googletrans","deep"], default="auto_chain")
    ap.add_argument("--zh-variant", choices=["cn","tw"], default="cn")
    ap.add_argument("--trace-translate", action="store_true")
    ap.add_argument("--cache-file", default=None, help="ë²ˆì—­ ê²°ê³¼ ìºì‹œ JSON ê²½ë¡œ")

    # ğŸ”¹ ì‹œë“œ ë³´ê°• ì˜µì…˜ (ì¹œì ˆ ë¡œê·¸ í¬í•¨)
    ap.add_argument("--region-seed", default=None, help="regions-addr-unique.json ê²½ë¡œ (ì˜µì…˜)")
    ap.add_argument("--category-seed", default=None, help="categories.json ê²½ë¡œ (ì˜µì…˜)")
    ap.add_argument("--seed-top-k", type=int, default=400, help="ì‹œë“œ ìƒìœ„ kê°œë§Œ ë³´ê°•")
    ap.add_argument("--seed-min-count", type=int, default=2, help="ì‹œë“œ ìµœì†Œ ë¹ˆë„")
    ap.add_argument("--no-seed-augment", action="store_true", help="ì‹œë“œ ë³´ê°• ë¹„í™œì„±í™”")

    args = ap.parse_args()
    show = (args.show.lower() == "true")
    show_name = (args.show_name_json.lower() == "true")
    show_report = (args.show_region_report.lower() == "true")
    do_save_region = (args.save_region.lower() == "true")
    do_save_category = (args.save_category.lower() == "true")

    # ë²ˆì—­ ì»¨íŠ¸ë¡¤ëŸ¬ + ìºì‹œ
    transctl = None
    if args.translate == "auto":
        transctl = TransCtl(
            provider=args.translate_provider,
            timeout=args.translate_timeout,
            max_calls=args.translate_max,
            zh_variant=args.zh_variant,
            trace=args.trace_translate,
        )
    load_trans_cache(args.cache_file)

    # ğŸ”¹ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì¹œì ˆ ë¡œê·¸ & ì‹œë“œ ë³´ê°•
    if args.no_seed_augment:
        print("[SEED] augmentation disabled (--no-seed-augment)")
    else:
        # Region seed
        if not (args.region_seed and os.path.exists(args.region_seed)):
            print(f"[SEED] region-seed not found -> skip augment ({_pretty_seed(args.region_seed)})")
        else:
            a, s = augment_jeju_from_region_seed(
                args.region_seed, args.translate, transctl, args.zh_variant,
                args.seed_top_k, args.seed_min_count
            )
            print(f"[SEED] region-seed applied: added={a}, skipped={s} (src={args.region_seed})")
        # Category seed
        if not (args.category_seed and os.path.exists(args.category_seed)):
            print(f"[SEED] category-seed not found -> skip augment ({_pretty_seed(args.category_seed)})")
        else:
            a, s = augment_glossary_from_category_seed(
                args.category_seed, args.translate, transctl, args.zh_variant,
                args.seed_top_k, args.seed_min_count
            )
            print(f"[SEED] category-seed applied: added={a}, skipped={s} (src={args.category_seed})")

    # ì…ë ¥ íŒŒì¼
    files = gather_files(args.files or [])
    if not files:
        print("[ERR] ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        end = time.perf_counter()
        print(f"[TIME] ì¢…ë£Œ: {now_local_str()}")
        print(f"[TIME] ì´ ì†Œìš”: {end-start:.3f}s")
        save_trans_cache(args.cache_file)
        sys.exit(2)

    # ì§‘ê³„ (ì§„í–‰ ë¡œê·¸ í¬í•¨)
    (regions_addr_per_shop, regions_road_per_shop,
     addr_triplet_counter, road_triplet_counter, cat_counter) = process_files(
        files, args.path, args.log_every, transctl
    )

    # í™”ë©´ ì¶œë ¥
    if show:
        print_regions_preview(regions_addr_per_shop, regions_road_per_shop, limit=12)
        print_categories_preview(cat_counter, limit=args.name_limit_cat)
        if show_name:
            print_regions_namejson_preview(addr_triplet_counter,
                                           translate_mode=args.translate,
                                           transctl=transctl,
                                           zh_variant=args.zh_variant,
                                           limit_per_level=args.name_limit_reg)
            print_categories_namejson_preview(cat_counter,
                                              translate_mode=args.translate,
                                              transctl=transctl,
                                              zh_variant=args.zh_variant,
                                              limit=args.name_limit_cat)
        if show_report:
            print_region_report(addr_triplet_counter,
                                limit_l1=args.report_limit_l1,
                                limit_l2=args.report_limit_l2,
                                limit_l3=args.report_limit_l3)

    # ì €ì¥
    if do_save_region:
        save_json(regions_addr_per_shop, args.out_addr_triplets)
        save_json(regions_road_per_shop, args.out_road_triplets)
        uniq_addr_list = [
            {"l1":k[0], "l2":k[1], "l3":k[2], "count":cnt}
            for k,cnt in sorted(addr_triplet_counter.items(), key=lambda x:(x[0][0],x[0][1],x[0][2]))
        ]
        uniq_road_list = [
            {"l1":k[0], "l2":k[1], "l3":k[2], "count":cnt}
            for k,cnt in sorted(road_triplet_counter.items(), key=lambda x:(x[0][0],x[0][1],x[0][2]))
        ]
        save_json(uniq_addr_list, args.out_addr_unique)
        save_json(uniq_road_list, args.out_road_unique)

    if do_save_category:
        cat_list = [{"category": k, "count": v} for k,v in sorted(cat_counter.items(), key=lambda x:(-x[1], x[0]))]
        save_json(cat_list, args.out_category)

    save_trans_cache(args.cache_file)
    end = time.perf_counter()
    print(f"[TIME] ì¢…ë£Œ: {now_local_str()}")
    print(f"[TIME] ì´ ì†Œìš”: {end-start:.3f}s")

if __name__ == "__main__":
    main()
